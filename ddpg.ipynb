{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "drlnd",
   "display_name": "drlnd",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### DDPG\n",
    "DDPG is closely related to DQN. It can be seen as DQN on continuous action space. The acto in DDPG learns a deterministic policy that maps states to a specific action. The critic is learned similar to DQN. It also applies soft update target network. To avoid divergence, DDPG slowly pulls in the weigts of learned networks into the target networks. This way, it improves stability of learning. Finally DDPG also applies Ornstein-Uhlenbeck process to explore continuous action space.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Architecture and Hyperparameters\n",
    "The actor network has 2 hidden layers, each goes through a RELU activation layer. The output layer has a tanh activation function. The critic network also has 2 hidden layers and an output layer. All activation function in critic network is RELU. Also to note that the input dimension of the critic network is the state plus and action space dimension.\n",
    "\n",
    "Hyperparameters (can be found in ddpg_agent.py file):\n",
    "\n",
    "- BUFFER_SIZE   # replay buffer size\n",
    "- BATCH_SIZE         # minibatch size\n",
    "- GAMMA             # discount factor\n",
    "- TAU              # for soft update of target parameters\n",
    "- LR_ACTOR         # learning rate of the actor \n",
    "- LR_CRITIC         # learning rate of the critic\n",
    "- WEIGHT_DECAY         # L2 weight decay\n",
    "- ACTOR_FC1_UNITS    # Number of units for the layer 1 in the actor model\n",
    "- ACTOR_FC2_UNITS    # Number of units for the layer 2 in the actor model\n",
    "- CRITIC_FCS1_UNITS  # Number of units for the layer 1 in the critic model\n",
    "- CRITIC_FC2_UNITS  # Number of units for the layer 2 in the critic model\n",
    "- BN_MODE          # Batch normalisation mode\n",
    "- ADD_OU_NOISE     # Add Ornstein-Uhlenbeck noise\n",
    "- MU            # Ornstein-Uhlenbeck noise parameter\n",
    "- THETA            # Ornstein-Uhlenbeck noise parameter\n",
    "- SIGMA             # Ornstein-Uhlenbeck noise parameter"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ddpg_agent import Agent\n",
    "from unityagents import UnityEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:unityagents:\n'Academy' started successfully!\nUnity Academy name: Academy\n        Number of Brains: 1\n        Number of External Brains : 1\n        Lesson number : 0\n        Reset Parameters :\n\t\tgoal_speed -> 1.0\n\t\tgoal_size -> 5.0\nUnity brain name: ReacherBrain\n        Number of Visual Observations (per agent): 0\n        Vector Observation space type: continuous\n        Vector Observation space size (per agent): 33\n        Number of stacked Vector Observation: 1\n        Vector Action space type: continuous\n        Vector Action space size (per agent): 4\n        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='Reacher.app')\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "action_size = brain.vector_action_space_size\n",
    "state_size = env_info.vector_observations.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddpg_agent import Agent\n",
    "\n",
    "# Agent default hyperparameters\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 128        # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR_ACTOR = 5e-4         # learning rate of the actor \n",
    "LR_CRITIC = 5e-4        # learning rate of the critic\n",
    "WEIGHT_DECAY = 0        # L2 weight decay\n",
    "ACTOR_FC1_UNITS = 128   # Number of units for the layer 1 in the actor model\n",
    "ACTOR_FC2_UNITS = 128   # Number of units for the layer 2 in the actor model\n",
    "CRITIC_FCS1_UNITS = 128 # Number of units for the layer 1 in the critic model\n",
    "CRITIC_FC2_UNITS = 128  # Number of units for the layer 2 in the critic model\n",
    "BN_MODE = 2             # Use Batch Norm. - 0=disabled, 1=BN before Activation, 2=BN after Activation (3, 4 are alt. versions of 1, 2)\n",
    "ADD_OU_NOISE = True     # Add Ornstein-Uhlenbeck noise\n",
    "MU = 0.                 # Ornstein-Uhlenbeck noise parameter\n",
    "THETA = 0.15            # Ornstein-Uhlenbeck noise parameter\n",
    "SIGMA = 0.2             # Ornstein-Uhlenbeck noise parameter\n",
    "\n",
    "\n",
    "def ddpg(state_size=state_size, action_size=action_size, random_seed=10, \n",
    "         actor_fc1_units=ACTOR_FC1_UNITS, actor_fc2_units=ACTOR_FC2_UNITS,\n",
    "         critic_fcs1_units=CRITIC_FCS1_UNITS, critic_fc2_units=CRITIC_FC2_UNITS,\n",
    "         buffer_size=BUFFER_SIZE, batch_size=BATCH_SIZE, bn_mode=BN_MODE,\n",
    "         gamma=GAMMA, tau=TAU, lr_actor=LR_ACTOR, lr_critic=LR_CRITIC, weight_decay=WEIGHT_DECAY,\n",
    "         add_ounoise=ADD_OU_NOISE, mu=MU, theta=THETA, sigma=SIGMA, n_episodes=5000, max_t=500):  \n",
    "    \n",
    "    # Instantiate the Agent\n",
    "    agent = Agent(state_size=state_size,action_size=action_size, random_seed=random_seed,\n",
    "                  actor_fc1_units=actor_fc1_units, actor_fc2_units=actor_fc2_units,\n",
    "                  critic_fcs1_units=critic_fcs1_units, critic_fc2_units=critic_fc2_units,\n",
    "                  buffer_size=buffer_size, batch_size=batch_size, bn_mode=bn_mode,\n",
    "                  gamma=gamma, tau=tau,\n",
    "                  lr_actor=lr_actor, lr_critic=lr_critic, weight_decay=weight_decay,\n",
    "                  add_ounoise=add_ounoise, mu=mu, theta=theta, sigma=sigma)\n",
    "                  \n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "\n",
    "    print(\"\\nStart training:\")\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        \n",
    "        # Reset the env and get the state (Single Agent)\n",
    "        env_info = env.reset(train_mode=True)[brain_name]     \n",
    "        state = env_info.vector_observations[0]\n",
    "        \n",
    "        # Reset the DDPG Agent (Reset the internal state (= noise) to mean mu)\n",
    "        agent.reset()\n",
    "        \n",
    "        # Reset the score \n",
    "        score = 0\n",
    "        \n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state)                   # select an action \n",
    "\n",
    "            env_info=env.step(action)[brain_name]        # send action to the environment\n",
    "            next_state = env_info.vector_observations[0] # get next state (Single Agent)\n",
    "            reward = env_info.rewards[0]                 # get reward (Single Agent)\n",
    "            done = env_info.local_done[0]                # see if episode finished (Single Agent)\n",
    "            \n",
    "            #if i_episode<2:\n",
    "            #    print(\"Debug: steps={} reward={} done={}\".format(t,reward,done))\n",
    "            \n",
    "            # Save experience in replay memory, and use random sample from buffer to learn\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            \n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                #print(\"Episode {} has terminated at step {}\".format(i_episode, t))\n",
    "                break \n",
    "        \n",
    "        # Save scores and compute average score over last 100 episodes\n",
    "        scores_deque.append(score)\n",
    "        scores.append(score)\n",
    "        avg_score = np.mean(scores_deque)\n",
    "        \n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}\\tScore: {:.2f}'.format(i_episode, avg_score, score), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "            torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "            # Early stop\n",
    "            if avg_score > 30:\n",
    "                print('\\rEnvironment solved in {} episodes with an Average Score of {:.2f}'.format(i_episode, avg_score))\n",
    "                return scores\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, avg_score))  \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "[INFO]ddpg constructor called with parameters: state_size=33 action_size=4 random_seed=10 actor_fc1_units=400 actor_fc2_units=300 critic_fcs1_units=400 critic_fc2_units=300 buffer_size=100000 batch_size=128 gamma=0.99 tau=0.001 bn_mode=0 lr_actor=0.0001 lr_critic=0.001 weight_decay=0 add_ounoise=True mu=0.0 theta=0.15 sigma=0.2 \n",
      "\n",
      "\n",
      "Start training:\n",
      "Episode 8\tAverage Score: 0.02\tScore: 0.00"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-4a8419b80771>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mddpg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-c878b8f12082>\u001b[0m in \u001b[0;36mddpg\u001b[0;34m(n_episodes, max_t, state_size, action_size, random_seed, actor_fc1_units, actor_fc2_units, critic_fcs1_units, critic_fc2_units, buffer_size, batch_size, bn_mode, gamma, tau, lr_actor, lr_critic, weight_decay, add_ounoise, mu, theta, sigma)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;31m# Save experience in replay memory, and use random sample from buffer to learn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/projects/ddpg/ddpg_agent.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/projects/ddpg/ddpg_agent.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, experiences, gamma)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;31m# ----------------------- update target networks ----------------------- #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoft_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_local\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtau\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoft_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_local\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtau\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/projects/ddpg/ddpg_agent.py\u001b[0m in \u001b[0;36msoft_update\u001b[0;34m(self, local_model, target_model, tau)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[1;32m    166\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtarget_param\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_param\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m             \u001b[0mtarget_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtau\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlocal_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtau\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtarget_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mOUNoise\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scores = ddpg(n_episodes=1500, max_t=1000, \n",
    "              actor_fc1_units=128, actor_fc2_units=128,\n",
    "              critic_fcs1_units=128, critic_fc2_units=128, bn_mode=2,\n",
    "              gamma=0.99, tau=1e-3, lr_actor=5e-4, lr_critic=5e-4, weight_decay=0.,\n",
    "              add_ounoise=True, mu=0., theta=0.15, sigma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "### Future work\n",
    "1. We can try hyperparameter tuning and change network architecture (num of nodes and num of layers etc).\n",
    "2. Instead of DDPG, we can explore other policy gradients based methods such as A2C, GAE, TD3."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}